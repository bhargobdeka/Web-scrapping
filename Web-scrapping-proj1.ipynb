{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"This is a simple Web scarpping exercise using Beautiful Soup in Python. We will be scrapping data like keywords, tables, photos etc... from a blog.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://montrealstartups.ca/five-biggest-montreal-ai-startups/\"\n",
    "r   = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Soup = bs(r.content)\n",
    "print(Soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Header = Soup.find(\"h1\", attrs={\"class\":\"case27-primary-text\"})\n",
    "Content = []\n",
    "Content = [Header.text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the author's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = Soup.find(\"div\", attrs={\"class\":\"abh_name fn name\"})\n",
    "# Adding to the content list\n",
    "Content += [body.text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting author's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describ = Soup.find(\"div\", attrs={\"class\":\"description note abh_description\"})\n",
    "Content += [Describ.text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5 Biggest Montreal AI Startups', 'Mark Yeramian', 'https://twitter.com/yeramianm']\n"
     ]
    }
   ],
   "source": [
    "links = Soup.select(\"div.abh_social a\")[0]\n",
    "Content += [links['href']]\n",
    "print(Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \"Top 5 AI Startups in Montreal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Element AI', 'Dialogue', 'Automat', 'Fluent.Ai', 'Keatext']\n"
     ]
    }
   ],
   "source": [
    "Main_content = []\n",
    "startup_list = []\n",
    "paragraphs = Soup.select(\"p>b\")\n",
    "para = [paragraph.text.strip() for paragraph in paragraphs]\n",
    "for s in para:\n",
    "    if s[0].isdigit():\n",
    "        result = ''.join([i for i in s if not i.isdigit()])\n",
    "        startup_list.append(result[2:len(result)])\n",
    "print(startup_list)\n",
    "Main_content += startup_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Other upcoming startups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hopper', 'Roof.ai', 'Imagia', 'C2RO Cloud Robotics']\n"
     ]
    }
   ],
   "source": [
    "other_list = []\n",
    "paragraphs = Soup.select(\"p>b\")\n",
    "para = [paragraph.text.strip() for paragraph in paragraphs]\n",
    "for s in para:\n",
    "    if not s[0].isdigit():\n",
    "           other_list.append(s)\n",
    "print(other_list)\n",
    "Main_content += other_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Element AI', 'Dialogue', 'Automat', 'Fluent.Ai', 'Keatext', 'Hopper', 'Roof.ai', 'Imagia', 'C2RO Cloud Robotics', 'https://www.elementai.com/', 'https://www.automat.ai/', 'https://www.fluent.ai/']\n"
     ]
    }
   ],
   "source": [
    "# Getting all the links\n",
    "all_links =[]\n",
    "hyperlinks = Soup.select(\"div.col-md-12.c27-content-wrapper\")[0]\n",
    "elements = hyperlinks.find_all(\"a\")\n",
    "for s in elements:\n",
    "    result = s['href']\n",
    "    all_links.append(result)\n",
    "\n",
    "# Extracting only the links \n",
    "result = [i for i in all_links if i.startswith('https://www.')]\n",
    "Main_content += result\n",
    "print(Main_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Writing all the data into a .csv file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data_webscrapping_proj1.csv','w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    wr.writerow(Main_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
